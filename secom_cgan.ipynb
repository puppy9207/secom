{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Keras 가 Tensorflow 를 벡엔드로 사용할 수 있도록 설정합니다.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# 실험을 재현하고 동일한 결과를 얻을 수 있는지 확인하기 위해 seed 를 설정합니다.\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-74c7b1e8b97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 데이터셋 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msecom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 424\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\software\\anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\software\\anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\software\\anaconda\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
    "secom = pd.read_csv(url, header=None, delim_whitespace=True)\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
    "y = pd.read_csv(url, header=None, usecols=[0], squeeze=True, delim_whitespace=True)\n",
    "# delim_whitespace = True : 빈 공간(' ')을 구분자로 인식하고 데이터 읽어옴\n",
    "# squeeze 만약 컬럼 하나만 읽어오면 데이터 구조를 Series로 읽어옴\n",
    "\n",
    "print('The dataset has {} observations/rows and {} variables/columns.'.format(secom.shape[0], secom.shape[1]))\n",
    "print('The majority class has {} observations, minority class {}.'.format(y[y == -1].size, y[y == 1].size))\n",
    "print('The dataset is imbalanced. The ratio of majority class to minority class is {%.2f}:1.' % (float(y[y == -1].size/y[y == 1].size)))\n",
    "\n",
    "dropthese = [i for i in range(590) if secom[i].std() == 0]\n",
    "secom_categorical = secom.drop(dropthese, axis = 1)\n",
    "print(secom_categorical.shape)\n",
    "secom_categorical.head()\n",
    "\n",
    "print('There are {} columns which have identical values recorded. We will drop these.'.format(len(dropthese)))\n",
    "print('The data set now has {} columns.'.format(secom_categorical.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list(map(lambda x: sum(secom[x].isnull()),range(secom_categorical.shape[1])))\n",
    "m_200thresh = list(filter(lambda i: (m[i] > 200), range(secom_categorical.shape[1])))\n",
    "print('The number of columns with more than 200 missing values: {}'.format(len(m_200thresh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secom_drop_200thresh = secom_categorical.dropna(subset=m_200thresh, axis=1)\n",
    "print('No. of columns after dropping columns with more than 200 missing entries: {}'.format(secom_drop_200thresh.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(secom_drop_200thresh, y, test_size = 0.2)\n",
    "# ndarray\n",
    "print(X_train.shape) # (1253, 414)\n",
    "print(X_test.shape) # (314, 414)\n",
    "print(y_train.shape) # (1253,)\n",
    "print(y_test.shape) # (314,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 파악을 위한 히스토그램 출력\n",
    "df_X_train = pd.DataFrame(X_train)\n",
    "feature_names = df_X_train.columns\n",
    "m = list((map(lambda i: sum(df_X_train[i].isnull()), feature_names)))\n",
    "plt.hist(m)\n",
    "plt.title(\"Distribution of missing values\")\n",
    "plt.xlabel(\"No. of missing values in a column\")\n",
    "plt.ylabel(\"Columns(count)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteriaList = [400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "for criteria in criteriaList :\n",
    "    columns_filtered = list(filter(lambda i: sum(df_X_train[i].isnull()) > criteria, df_X_train.columns))\n",
    "    print('The number of columns with more than {:>4d} missing values(about {}%): {:>2d}'.format(criteria, int((criteria/1253)*100), len(columns_filtered)))\n",
    "\n",
    "# fill_NaN_by_Gaussian 함수를 적용하기 위해 데이터 형변환 (ndarray -> DataFrame)\n",
    "df_X_train = pd.DataFrame(X_train)\n",
    "df_X_test = pd.DataFrame(X_test)\n",
    "print(df_X_train.shape)\n",
    "print(df_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN_by_Gaussian(df_X_train, df_X_test):\n",
    "    \"\"\"결측치를 해당 열의 가우시안 분포를 따르는 난수로 대체하는 함수이다.\n",
    "       단, test셋의 각 열은 train셋의 각 열의 가우시안 분포를 따른다고 가정한다.\n",
    "       따라서, test셋은 train셋의 mean, std를 사용한다.                      \"\"\"\n",
    "\n",
    "    for column in df_X_train.columns.values:\n",
    "        mean = df_X_train[column].mean()\n",
    "        std = df_X_train[column].std()\n",
    "\n",
    "        X_train_NaN_size = sum(df_X_train[column].isnull())\n",
    "        X_test_NaN_size = sum(df_X_test[column].isnull())\n",
    "\n",
    "        df_X_train.loc[df_X_train[column].isnull(), column] = np.random.normal(mean, std, size=X_train_NaN_size)\n",
    "        df_X_test.loc[df_X_test[column].isnull(), column] = np.random.normal(mean, std, size=X_test_NaN_size)\n",
    "\n",
    "    return (df_X_train, df_X_test)\n",
    "# main\n",
    "df_X_train, df_X_test = fill_NaN_by_Gaussian(df_X_train, df_X_test)\n",
    "print(df_X_train.shape)\n",
    "print(df_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준 정규 분포로 Normalization\n",
    "# train 데이터의 각 열에 적용한 평균과 표준편차를 test 데이터의 각 열에 동일하게 적용함\n",
    "def standardProcess(df_X_train, df_X_test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    std_scale_parameters = scaler.fit(df_X_train.values) # train 데이터의 평균과 표준편차로 파라미터를 피팅한 후 저장, test 데이터에도 똑같은 파라미터를 적용하기 위함\n",
    "\n",
    "    scaled_X_train= std_scale_parameters.transform(df_X_train) # [n_samples, n_features]의 크기로 반환, [1253,474]\n",
    "    scaled_X_test= std_scale_parameters.transform(df_X_test) # [n_samples, n_features]의 크기로 반환, [314,474]\n",
    "\n",
    "    scaled_df_X_train = pd.DataFrame(scaled_X_train, index = df_X_train.index, columns = df_X_train.columns) # 데이터 프레임으로 변환, 기존의 index, columns 사용\n",
    "    scaled_df_X_test = pd.DataFrame(scaled_X_test, index =df_X_test.index, columns = df_X_test.columns) # 데이터 프레임으로 변환, 기존의 index, columns 사용\n",
    "\n",
    "    return scaled_df_X_train, scaled_df_X_test\n",
    "\n",
    "def minmaxProcess(df_X_train, df_X_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    mms_scale_params = scaler.fit(df_X_train.values)\n",
    "\n",
    "    scaled_X_train = mms_scale_params.transform(df_X_train)\n",
    "    scaled_X_test = mms_scale_params.transform(df_X_test)\n",
    "\n",
    "    scaled_df_X_train = pd.DataFrame(scaled_X_train, index= df_X_train.index, columns= df_X_train.columns)\n",
    "    scaled_df_X_test = pd.DataFrame(scaled_X_test, index= df_X_test.index, columns= df_X_test.columns)\n",
    "\n",
    "    return scaled_df_X_train, scaled_df_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = input(\"사용할 Scaler 함수를 입력 / mms = MinMaxScaler / std = StandardScaler\\n\")\n",
    "if select == \"std\":\n",
    "    print(\"선택한 함수는 Standard Scaler\")\n",
    "    scaled_df_X_train, scaled_df_X_test = standardProcess(df_X_train,df_X_test)\n",
    "elif select == \"mms\":\n",
    "    print(\"선택한 함수는 MinMaxScaler\")\n",
    "    scaled_df_X_train, scaled_df_X_test = minmaxProcess(df_X_train, df_X_test)\n",
    "else:\n",
    "    print(\"다른 값 입력으로 default로 Standard Scaler를 사용합니다.\")\n",
    "    scaled_df_X_train, scaled_df_X_test = standardProcess(df_X_train, df_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# PCA 인스턴스 객체를 생성.\n",
    "pca = PCA(n_components=35)\n",
    "\n",
    "# 생성된 PCA 인스턴스 객체에 scaled_df_X_train를 Fitting.\n",
    "# 주의: fit은 오직 scaled_df_X_train에만 적용.\n",
    "# 동일한 파라미터를 scaled_df_X_test에 적용하기 위함.\n",
    "pca.fit(scaled_df_X_train)\n",
    "\n",
    "# transform하게 되면  ndarray로 반환하기 때문에 별도의 DataFrame 형변환 필요.\n",
    "X_train_after_PCA = pca.transform(scaled_df_X_train)\n",
    "X_test_after_PCA = pca.transform(scaled_df_X_test)\n",
    "\n",
    "# PCA 수행 후의 X_train을 DataFrame으로 형변환\n",
    "df_X_train_after_PCA = pd.DataFrame(data=X_train_after_PCA, index=scaled_df_X_train.index)\n",
    "# PCA 수행 후의 X_test을 DataFrame으로 형변환\n",
    "df_X_test_after_PCA = pd.DataFrame(data=X_test_after_PCA, index=scaled_df_X_test.index)\n",
    "\n",
    "print(df_X_train_after_PCA.shape[0],df_X_train_after_PCA.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래부턴 CGAN 코드\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from keras.layers import Input, Flatten, Embedding, multiply, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "# latent space dimension\n",
    "latent_dim = 100\n",
    "secom_dim = df_X_train_after_PCA.shape[1]\n",
    "init = initializers.RandomNormal(stddev=0.02)\n",
    "# Generator network\n",
    "generator = Sequential()\n",
    "# Input layer and hidden layer 1\n",
    "generator.add(Dense(76, input_shape=(latent_dim,),\n",
    "                    kernel_initializer=init))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU(0.2))\n",
    "# Hidden layer 2\n",
    "generator.add(Dense(152))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(BatchNormalization())\n",
    "# Hidden layer 3\n",
    "generator.add(Dense(304))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(BatchNormalization())\n",
    "# Output layer\n",
    "generator.add(Dense(secom_dim, activation='tanh'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding condition in input layer\n",
    "num_classes = 2\n",
    "# Create label embeddings\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Embedding(num_classes, latent_dim)(label)\n",
    "label_embedding = Flatten()(label_embedding)\n",
    "# latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "# Merge inputs (z x label)\n",
    "input_generator = multiply([z, label_embedding])\n",
    "\n",
    "secom_generate = generator(input_generator)\n",
    "# Generator with condition input\n",
    "generator = Model([z, label], secom_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "discriminator = Sequential()\n",
    "\n",
    "# Input layer and hidden layer 1\n",
    "discriminator.add(Dense(76, input_shape=(secom_dim,),\n",
    "                        kernel_initializer=init))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "# Hidden layer 2\n",
    "discriminator.add(Dense(152))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "discriminator.add(Dense(304))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "# Output layer\n",
    "discriminator.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding condition in input layer\n",
    "\n",
    "# Create label embeddings\n",
    "label_d = Input(shape=(1,), dtype='int32')\n",
    "label_embedding_d = Embedding(num_classes, secom_dim)(label_d)\n",
    "label_embedding_d = Flatten()(label_embedding_d)\n",
    "\n",
    "img_d = Input(shape=(secom_dim,))\n",
    "\n",
    "# Merge inputs (img x label)\n",
    "input_discriminator = multiply([img_d, label_embedding_d])\n",
    "\n",
    "# Output image\n",
    "validity = discriminator(input_discriminator)\n",
    "\n",
    "# Discriminator with condition input\n",
    "discriminator = Model([img_d, label_d], validity)\n",
    "\n",
    "# Optimizer\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "discriminator.compile(opt, loss='binary_crossentropy',\n",
    "                      metrics=['binary_accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity = discriminator([generator([z, label]), label])\n",
    "\n",
    "d_g = Model([z, label], validity)\n",
    "\n",
    "d_g.compile(opt, loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 64\n",
    "smooth = 0.1\n",
    "\n",
    "real = np.ones(shape=(batch_size, 1))\n",
    "fake = np.zeros(shape=(batch_size, 1))\n",
    "\n",
    "d_loss = []\n",
    "d_g_loss = []\n",
    "np_X_train = df_X_train_after_PCA.as_matrix()\n",
    "np_y_train = pd.DataFrame(y_train).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_y_train_df = []\n",
    "for i in range(100):\n",
    "    fake_y_train_df.append(2)\n",
    "fake_y_train_df = pd.DataFrame(fake_y_train_df)\n",
    "tsneEmbedded = TSNE(n_components=2,perplexity=30,learning_rate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs + 1):\n",
    "    for i in range(len(np_X_train) // batch_size):\n",
    "        # Train Discriminator weights\n",
    "        discriminator.trainable = True\n",
    "\n",
    "        # Real samples\n",
    "        X_batch = np_X_train[i * batch_size:(i + 1) * batch_size]\n",
    "        real_labels = np_y_train[i * batch_size:(i + 1) * batch_size].reshape(-1, 1)\n",
    "        d_loss_real = discriminator.train_on_batch(x=[X_batch, real_labels],\n",
    "                                                   y=real * (1 - smooth))\n",
    "\n",
    "        # Fake Samples\n",
    "        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
    "        random_labels = np.random.choice([-1,1], batch_size).reshape(-1, 1)\n",
    "        X_fake = generator.predict_on_batch([z, random_labels])\n",
    "\n",
    "        d_loss_fake = discriminator.train_on_batch(x=[X_fake, random_labels], y=fake)\n",
    "\n",
    "        # Discriminator loss\n",
    "        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
    "\n",
    "        # Train Generator weights\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
    "        random_labels = np.random.choice([-1,1], batch_size).reshape(-1, 1)\n",
    "        d_g_loss_batch = d_g.train_on_batch(x=[z, random_labels], y=real)\n",
    "\n",
    "        print(\n",
    "            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (\n",
    "            e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, d_g_loss_batch[0]),\n",
    "            100 * ' ',\n",
    "            end='\\r'\n",
    "        )\n",
    "\n",
    "    d_loss.append(d_loss_batch)\n",
    "    d_g_loss.append(d_g_loss_batch[0])\n",
    "    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], d_g_loss[-1]), 100 * ' ')\n",
    "    if e % 25 == 0:\n",
    "        samples = 100\n",
    "        z = np.random.normal(loc=0, scale=1, size=(samples, latent_dim))\n",
    "        labels = np.random.choice([1],samples).reshape(-1, 1)\n",
    "        x_fake = generator.predict([z, labels])\n",
    "        x_fake = pd.DataFrame(x_fake)\n",
    "        X_y_PCA = pd.concat([df_X_train_after_PCA,y_train],axis=1)\n",
    "        X_y_fake = pd.concat([x_fake,fake_y_train_df],axis=1)\n",
    "        X_y_PCA = X_y_PCA.values\n",
    "        X_y_except = []\n",
    "        for i in X_y_PCA:\n",
    "                for j in i:\n",
    "                    if j==1:\n",
    "                        X_y_except.append(i)\n",
    "        X_y_except = pd.DataFrame(X_y_except)\n",
    "        X_y_fake = X_y_fake.values\n",
    "        X_y_fake = pd.DataFrame(X_y_fake)\n",
    "        X_y_concat = pd.concat([X_y_except,X_y_fake],axis=0)\n",
    "        X_y_concat.to_csv(\"csvdirectory/csv%d.csv\"%e)\n",
    "        df1 = X_y_concat.iloc[:,0:76]\n",
    "        df2 = list(X_y_concat.iloc[:,-1])\n",
    "        X_embedded = tsneEmbedded.fit_transform(df1)\n",
    "        xs = X_embedded[:,0]\n",
    "        ys = X_embedded[:,1]\n",
    "        fig = plt.gcf()\n",
    "        cntb = True\n",
    "        cntg = True\n",
    "        for i in range(len(xs)):\n",
    "            if df2[i]== 1.0:\n",
    "                if cntb==True:\n",
    "                    plt.scatter(xs[i],ys[i],label=\"default\",c=\"blue\")\n",
    "                    cntb = False\n",
    "                else:\n",
    "                    plt.scatter(xs[i],ys[i],c=\"blue\")\n",
    "            elif df2[i]==2.0:\n",
    "                if cntg ==True:\n",
    "                    plt.scatter(xs[i],ys[i],label=\"cGAN\",c=\"yellow\")\n",
    "                    cntg = False\n",
    "                else:\n",
    "                    plt.scatter(xs[i],ys[i],c=\"yellow\")\n",
    "        plt.legend()\n",
    "        plt.title(\"%d\"%e)\n",
    "        fig.savefig('csvdirectory/%dfig.png'%e)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
